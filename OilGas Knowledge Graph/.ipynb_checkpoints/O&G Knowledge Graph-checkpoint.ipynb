{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil and Gas Knowledge Graph\n",
    "\n",
    "Código baseado no tutorial de Chris Thornton, [Auto-Generated Knowledge Graphs](https://towardsdatascience.com/auto-generated-knowledge-graphs-92ca99a81121)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\KGraph\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\KGraph\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\KGraph\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Importando os bibliotecas\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o dataset  \n",
    "\n",
    "inicialmente vamos utilizar o corpus público do [Petrolês](https://petroles.puc-rio.ai/) sem as teses do IBICT, com o preprocessamento disponibilizado (apenas eliminação de ruídos, números e caracteres especiais). Ao final, a escolha do corpus deve ser reavaliada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'corpusPublico(sem IBICT)-SemProcessamento.txt'\n",
    "text = open(file, \"r\", encoding='utf-8').read()\n",
    "text = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de sentenças:  4479566\n"
     ]
    }
   ],
   "source": [
    "print('Número de sentenças: ', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Linguistics & NLP Algorithms  \n",
    "\"Knowledge graphs can be constructed automatically from text using part-of-speech and dependency parsing. The extraction of entity pairs from grammatical patterns is fast and scalable to large amounts of text using NLP library SpaCy.\n",
    "The following function defines entity pairs as entities/noun chunks with subject — object dependencies connected by a root verb. Other rules-of-thumb can be used to produce different types of connections. This kind of connection can be referred to as a subject-predicate-object triple.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando a versão reduzida do modelo da língua portuguesa\n",
    "# Depois testar outras versões\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')  \n",
    "# neuralcoref.add_to_pipe(nlp) #usar apenas em textos em inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pesquisas relacionadas com hidratos de metano no entanto ainda são muito preliminares e não se sabe ainda os efeitos que a extração destas formações poderá ter no ciclo natural do carbono  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nlp(text[1001])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "\n",
    "def get_entity_pairs(text, coref=True):\n",
    "    # preprocess text\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "\n",
    "    def refine_ent(ent, sent):\n",
    "        unwanted_tokens = (\n",
    "            'PRON',  # pronouns\n",
    "            'PART',  # particle\n",
    "            'DET',  # determiner\n",
    "            'SCONJ',  # subordinating conjunction\n",
    "            'PUNCT',  # punctuation\n",
    "            'SYM',  # symbol\n",
    "            'X',  # other\n",
    "        )\n",
    "        ent_type = ent.ent_type_  # get entity type\n",
    "        if ent_type == '':\n",
    "            ent_type = 'NOUN_CHUNK'\n",
    "            ent = ' '.join(str(t.text) for t in\n",
    "                           nlp(str(ent)) if t.pos_\n",
    "                           not in unwanted_tokens and t.is_stop == False)\n",
    "        elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "            refined = ''\n",
    "            for i in range(len(sent) - ent.i):\n",
    "                if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                    refined += ' ' + str(ent.nbor(i))\n",
    "                else:\n",
    "                    ent = refined.strip()\n",
    "                    break\n",
    "\n",
    "        return ent, ent_type\n",
    "\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    ent_pairs = []\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span, attrs={'tag': span.root.tag,\n",
    "                                            'dep': span.root.dep}) for span in spans]\n",
    "        deps = [token.dep_ for token in sent]\n",
    "\n",
    "        # limit our example to simple sentences with one subject and object\n",
    "        if (deps.count('obj') + deps.count('dobj')) != 1\\\n",
    "                or (deps.count('subj') + deps.count('nsubj')) != 1:\n",
    "            continue\n",
    "\n",
    "        for token in sent:\n",
    "            if token.dep_ not in ('obj', 'dobj'):  # identify object nodes\n",
    "                continue\n",
    "            subject = [w for w in token.head.lefts if w.dep_\n",
    "                       in ('subj', 'nsubj')]  # identify subject nodes\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                # identify relationship by root dependency\n",
    "                relation = [w for w in token.ancestors if w.dep_ == 'ROOT']\n",
    "                if relation:\n",
    "                    relation = relation[0]\n",
    "                    # add adposition or particle to relationship\n",
    "                    if relation.nbor(1).pos_ in ('ADP', 'PART'):\n",
    "                        relation = ' '.join((str(relation), str(relation.nbor(1))))\n",
    "                else:\n",
    "                    relation = 'unknown'\n",
    "\n",
    "                subject, subject_type = refine_ent(subject, sent)\n",
    "                token, object_type = refine_ent(token, sent)\n",
    "\n",
    "                ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                  str(subject_type), str(object_type)])\n",
    "\n",
    "    ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(ent) == '' for ent in sublist)]\n",
    "    pairs = pd.DataFrame(ent_pairs, columns=['subject', 'relation', 'object',\n",
    "                                             'subject_type', 'object_type'])\n",
    "    print('Entity pairs extracted:', str(len(ent_pairs)))\n",
    "\n",
    "    return pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
